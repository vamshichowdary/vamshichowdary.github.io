---
title: "A Study of Generalization in Deep Neural Networks"
collection: Reports
permalink: /publication/msthesis
excerpt: 'Understanding generalization of DNNs by investigating the role of different attributes of DNNs, both structural - such as width, depth, kernel parameters, skip connections, etc - as well as functional - such as intermediate feature representations, receptive fields of CNN kernels'
date: 2021-09
venue: 'University of California, Santa Barbara'
paperurl: 'https://www.proquest.com/docview/2604320736/abstract/967E2748BD3A4BFAPQ/1?accountid=14522'
citation: 'Vamshi C Madala. 2021. A study of generalization in deep neural networks. Ann Arbor: University of California, Santa Barbara.'
---
We study generalization in deep learning by appealing to complexity measures originally developed in approximation and information theory. While these concepts are challenged by the high-dimensional and data-defined nature of deep learning, we show that simple vector quantization approaches such as PCA, GMMs, and SVMs capture their spirit when applied layer-wise to deep extracted features giving rise to relatively inexpensive complexity measures that correlate well with generalization performance. We discuss our results in 2020 NeurIPS PGDL challenge.

Download paper here: [Link1](https://www.proquest.com/docview/2604320736/abstract/967E2748BD3A4BFAPQ/1?accountid=14522)

Recommended citation: 'Vamshi C Madala. 2021. A study of generalization in deep neural networks. Ann Arbor: University of California, Santa Barbara.'
